{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from utils import DataManager\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from probes import LRProbe, MMProbe, CCSProbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "model_size = '13B'\n",
    "layer = 16 # layer from which to extract activations\n",
    "split = 0.9\n",
    "\n",
    "device = 'cuda:0' if t.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing generalization matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'probes.LRProbe'>\n",
      "Layer 2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m dm \u001b[38;5;241m=\u001b[39m DataManager()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m medley:\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m val_datasets:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dataset \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m medley:\n",
      "File \u001b[0;32m~/mechinterp/geometry-of-truth/utils.py:93\u001b[0m, in \u001b[0;36mDataManager.add_dataset\u001b[0;34m(self, dataset_name, model_size, layer, label, split, seed, center, scale, device, max_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset_name, model_size, layer, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, center\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, max_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03m    Add a dataset to the DataManager.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    label : which column of the csv file to use as the labels.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m    If split is not None, gives the train/val split proportion. Uses seed for reproducibility.\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     acts \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_acts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     95\u001b[0m     labels \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mTensor(df[label]\u001b[38;5;241m.\u001b[39mvalues)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/mechinterp/geometry-of-truth/utils.py:53\u001b[0m, in \u001b[0;36mcollect_acts\u001b[0;34m(dataset_name, model_size, layer, center, scale, device)\u001b[0m\n\u001b[1;32m     51\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macts\u001b[39m\u001b[38;5;124m'\u001b[39m, model_size, dataset_name)\n\u001b[1;32m     52\u001b[0m activation_files \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 53\u001b[0m acts \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mACTS_BATCH_SIZE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactivation_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mACTS_BATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m acts \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcat(acts, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m center:\n",
      "File \u001b[0;32m~/mechinterp/geometry-of-truth/utils.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macts\u001b[39m\u001b[38;5;124m'\u001b[39m, model_size, dataset_name)\n\u001b[1;32m     52\u001b[0m activation_files \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 53\u001b[0m acts \u001b[38;5;241m=\u001b[39m [\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayer_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlayer\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, ACTS_BATCH_SIZE \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(activation_files), ACTS_BATCH_SIZE)]\n\u001b[1;32m     54\u001b[0m acts \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcat(acts, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m center:\n",
      "File \u001b[0;32m~/virtual/lib/python3.11/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/virtual/lib/python3.11/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/virtual/lib/python3.11/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/virtual/lib/python3.11/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/virtual/lib/python3.11/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/virtual/lib/python3.11/site-packages/torch/serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/virtual/lib/python3.11/site-packages/torch/serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_medlies  = [\n",
    "    # ['stereoset_gender', 'stereoset_gender_dev', 'stereoset_race', 'stereoset_race_dev'],\n",
    "    #['stereoset_race'],\n",
    "    ['bbq_race_statements'],\n",
    "    #['bbq_race_targets'],\n",
    "]\n",
    "\n",
    "val_datasets = [\n",
    "    # 'stereoset_race',\n",
    "    # 'stereoset_gender',\n",
    "    # 'stereoset_race',\n",
    "    'bbq_race_statements',\n",
    "    'bbq_race_targets',\n",
    "    #'bbq_race_gender',\n",
    "    #'bbq_race_ses',\n",
    "]\n",
    "\n",
    "\n",
    "def to_str(l):\n",
    "    return '+'.join(l)\n",
    "\n",
    "ProbeClasses = [\n",
    "    LRProbe, \n",
    "    MMProbe, \n",
    "    ]\n",
    "\n",
    "accs = {str(probe_class) : {to_str(train_medley) : {val: {} for val in val_datasets}\n",
    "                            for train_medley in train_medlies}\n",
    "        for probe_class in ProbeClasses}\n",
    "\n",
    "seeds = random.sample(range(0, 100000), 3)\n",
    "\n",
    "for ProbeClass in ProbeClasses:\n",
    "    #print(str(ProbeClass))\n",
    "    for medley in train_medlies:\n",
    "        for layer in range(2, 14):\n",
    "            for seed in seeds:\n",
    "                #print(f\"Layer {layer}\")\n",
    "                # set up data\n",
    "                dm = DataManager()\n",
    "                for dataset in medley:\n",
    "                    dm.add_dataset(dataset, model_size, layer, split=split, seed=seed, center=True, device=device)\n",
    "                for dataset in val_datasets:\n",
    "                    if dataset not in medley:\n",
    "                        dm.add_dataset(dataset, model_size, layer, split=None, center=True, device=device)\n",
    "        \n",
    "                # train probe\n",
    "                train_acts, train_labels = dm.get('train')\n",
    "                probe = ProbeClass.from_data(train_acts, train_labels, device=device)\n",
    "        \n",
    "                # evaluate\n",
    "                for val_dataset in val_datasets:\n",
    "                    if layer not in accs[str(ProbeClass)][to_str(medley)][val_dataset]:\n",
    "                        accs[str(ProbeClass)][to_str(medley)][val_dataset][layer] = []\n",
    "                    #print(val_dataset)\n",
    "                    if val_dataset in medley:\n",
    "                        acts, labels = dm.data['val'][val_dataset]\n",
    "                        accs[str(ProbeClass)][to_str(medley)][val_dataset][layer].append(\n",
    "                            (probe.pred(acts, iid=True) == labels\n",
    "                            ).float().mean().item())\n",
    "                    else:\n",
    "                        acts, labels = dm.data[val_dataset]\n",
    "                        accs[str(ProbeClass)][to_str(medley)][val_dataset][layer].append(\n",
    "                            (probe.pred(acts, iid=False) == labels\n",
    "                        ).float().mean().item())\n",
    "\n",
    "lr_mm_accs = accs.copy()\n",
    "# lr_mm_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_statements_acc = lr_mm_accs[\"<class 'probes.LRProbe'>\"]['bbq_race_targets']['bbq_race_targets']\n",
    "mm_statements_acc = lr_mm_accs[\"<class 'probes.MMProbe'>\"]['bbq_race_targets']['bbq_race_targets']\n",
    "# compute mean and errors\n",
    "\n",
    "means = []\n",
    "errs = []\n",
    "for acc_df in [lr_statements_acc, mm_statements_acc]:\n",
    "    k, values = acc_df.keys(), acc_df.values()\n",
    "    mean = [np.mean(t) for t in values]\n",
    "    means.append(mean)\n",
    "    #err = [np.max(t) - np.min(t) for t in values]\n",
    "    err = [np.std(t) for t in values]\n",
    "    errs.append(err)\n",
    "\n",
    "plt.plot(lr_statements_acc.keys(), means[0])\n",
    "plt.plot(mm_statements_acc.keys(), means[1])\n",
    "plt.legend(['Linear', 'Mass-mean'])\n",
    "plt.show()\n",
    "    \n",
    "# plt.errorbar(lr_statements_acc.keys(), means[0], yerr=errs[0])\n",
    "# plt.errorbar(mm_statements_acc.keys(), means[1], yerr=errs[1])\n",
    "\n",
    "lr_values = [x for x in lr_statements_acc.values()]\n",
    "plt.boxplot(lr_values, positions=list(lr_statements_acc.keys()))\n",
    "plt.title('Layerwise linear probe accuracy on validation set')\n",
    "plt.show()\n",
    "mm_values = [x for x in mm_statements_acc.values()]\n",
    "plt.boxplot(mm_values, positions=list(mm_statements_acc.keys()))\n",
    "plt.title('Layerwise probe mass mean accuracy on validation set')\n",
    "plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "for ax_idx, ax in enumerate(axes):\n",
    "    if ax_idx == 0:\n",
    "        ax.set_title(\"Logistic regression\")\n",
    "        ax_accs = lr_mm_accs[str(LRProbe)]\n",
    "    elif ax_idx == 1:\n",
    "        ax.set_title(\"Mass mean\")\n",
    "        ax_accs = lr_mm_accs[str(MMProbe)]\n",
    "    else:\n",
    "        continue # do this outside of the loop\n",
    "\n",
    "    \n",
    "    grid = [ [] for _ in val_datasets]\n",
    "\n",
    "    for i, val_dataset in enumerate(val_datasets):\n",
    "        for medley in train_medlies:\n",
    "            if medley == ['gender']:\n",
    "                continue # treat likely as a baseline\n",
    "            grid[i].append(ax_accs[to_str(medley)][val_dataset])\n",
    "\n",
    "\n",
    "    ax.imshow(grid, vmin=0, vmax=1)\n",
    "    for i in range(len(grid)):\n",
    "        for j in range(len(grid[0])):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d}', ha='center', va='center')\n",
    "\n",
    "    ax.set_xticks(range(len(train_medlies) - 1))\n",
    "    ax.set_xticklabels([to_str(medley) for medley in train_medlies[:-1]], rotation=90)\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# ax = axes[2]\n",
    "# ax.set_title(\"CCS\")\n",
    "# grid = [ [] for _ in val_datasets]\n",
    "# for i, val_dataset in enumerate(val_datasets):\n",
    "#     for medley in ccs_medlies:\n",
    "#         grid[i].append(ccs_accs[to_str(medley)][val_dataset])\n",
    "# ax.imshow(grid, vmin=0, vmax=1)\n",
    "# for i in range(len(grid)):\n",
    "#     for j in range(len(grid[0])):\n",
    "#         ax.text(j, i, f'{round(grid[i][j] * 100):2d}', ha='center', va='center')\n",
    "\n",
    "# ax.set_xticks(range(len(ccs_medlies)))\n",
    "# ax.set_xticklabels([to_str(medley) for medley in ccs_medlies], rotation=90)\n",
    "# ax.set_yticks([])\n",
    "\n",
    "\n",
    "ax = axes[2]\n",
    "ax.set_title(\"Baselines\")\n",
    "grid = [ [\n",
    "    lr_mm_accs[str(LRProbe)]['gender'][val_dataset],\n",
    "    lr_mm_accs[str(MMProbe)]['gender'][val_dataset],\n",
    "    # few_shot_accs[i],\n",
    "    # oracle_accs[str(LRProbe)][i]\n",
    "    ] for i, val_dataset in enumerate(val_datasets) \n",
    "]\n",
    "\n",
    "ax.imshow(grid, vmin=0, vmax=1)\n",
    "for i in range(len(grid)):\n",
    "    for j in range(len(grid[0])):\n",
    "        ax.text(j, i, f'{round(grid[i][j] * 100):2d}', ha='center', va='center')\n",
    "\n",
    "ax.set_xticks(range(2))\n",
    "ax.set_xticklabels(['LR on gender', 'MM on gender'], rotation=90)\n",
    "ax.set_yticks([])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.hlines([1.5, 3.5, 5.5, 7.5], *ax.get_xlim(), linewidth=3, color='black')\n",
    "    ax.vlines([1.5], *ax.get_ylim(), linewidth=3, color='black')\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_yticks(range(len(val_datasets)))\n",
    "ax.set_yticklabels(val_datasets)\n",
    "\n",
    "\n",
    "plt.colorbar(axes[0].images[0], ax=axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccs_medlies = [\n",
    "    # ['cities', 'neg_cities'],\n",
    "    # ['larger_than', 'smaller_than'],\n",
    "    ['gender'],\n",
    "    ['race'],\n",
    "]\n",
    "\n",
    "accs = {to_str(medley) : {} for medley in ccs_medlies}\n",
    "\n",
    "for medley in ccs_medlies:\n",
    "    dm = DataManager()\n",
    "    for dataset in medley:\n",
    "        dm.add_dataset(dataset, model_size, layer, split=split, seed=seed, center=True, device=device)\n",
    "    for dataset in val_datasets:\n",
    "        if dataset not in medley:\n",
    "            dm.add_dataset(dataset, model_size, layer, split=None, center=True, device=device)\n",
    "    \n",
    "    train_acts, train_labels = dm.data['train'][medley[0]]\n",
    "    train_neg_acts, _ = dm.data['train'][medley[1]]\n",
    "    probe = CCSProbe.from_data(train_acts, train_neg_acts, train_labels, device=device)\n",
    "\n",
    "    for val_dataset in val_datasets:\n",
    "        if val_dataset in medley:\n",
    "            acts, labels = dm.data['val'][val_dataset]\n",
    "        else:\n",
    "            acts, labels = dm.data[val_dataset]\n",
    "        accs[to_str(medley)][val_dataset] = (\n",
    "            probe.pred(acts) == labels\n",
    "        ).float().mean().item()\n",
    "    \n",
    "ccs_accs = accs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve few-shot results\n",
    "# !! must first run few_shot.py to populate experimental_outputs/few_shot_results.json !!\n",
    "import json\n",
    "with open('experimental_outputs/few_shot_results.json', 'r') as f:\n",
    "    few_shot_results = json.load(f)\n",
    "few_shot_accs = []\n",
    "for dataset in val_datasets:\n",
    "    all_accs = [d['acc'] for d in few_shot_results if d['dataset'] == dataset]\n",
    "    few_shot_accs.append(max(all_accs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get oracle probe results\n",
    "oracle_accs = {str(probe_class) : [] for probe_class in ProbeClasses}\n",
    "for ProbeClass in ProbeClasses:\n",
    "    for dataset in val_datasets:\n",
    "        dm = DataManager()\n",
    "        dm.add_dataset(dataset, model_size, layer, split=split, seed=seed, device=device)\n",
    "        acts, labels = dm.get('train')\n",
    "        probe = ProbeClass.from_data(acts, labels, device=device)\n",
    "\n",
    "        acts, labels = dm.data['val'][dataset]\n",
    "        acc = (probe(acts, iid=True).round() == labels).float().mean().item()\n",
    "        oracle_accs[str(ProbeClass)].append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(30, 10))\n",
    "for ax_idx, ax in enumerate(axes):\n",
    "    if ax_idx == 0:\n",
    "        ax.set_title(\"Logistic regression\")\n",
    "        ax_accs = lr_mm_accs[str(LRProbe)]\n",
    "    elif ax_idx == 1:\n",
    "        ax.set_title(\"Mass mean\")\n",
    "        ax_accs = lr_mm_accs[str(MMProbe)]\n",
    "    else:\n",
    "        continue # do this outside of the loop\n",
    "\n",
    "    \n",
    "    grid = [ [] for _ in val_datasets]\n",
    "\n",
    "    for i, val_dataset in enumerate(val_datasets):\n",
    "        for medley in train_medlies:\n",
    "            if medley == ['likely']:\n",
    "                continue # treat likely as a baseline\n",
    "            grid[i].append(ax_accs[to_str(medley)][val_dataset])\n",
    "\n",
    "\n",
    "    ax.imshow(grid, vmin=0, vmax=1)\n",
    "    for i in range(len(grid)):\n",
    "        for j in range(len(grid[0])):\n",
    "            ax.text(j, i, f'{round(grid[i][j] * 100):2d}', ha='center', va='center')\n",
    "\n",
    "    ax.set_xticks(range(len(train_medlies) - 1))\n",
    "    ax.set_xticklabels([to_str(medley) for medley in train_medlies[:-1]], rotation=90)\n",
    "    ax.set_yticks([])\n",
    "\n",
    "ax = axes[2]\n",
    "ax.set_title(\"CCS\")\n",
    "grid = [ [] for _ in val_datasets]\n",
    "for i, val_dataset in enumerate(val_datasets):\n",
    "    for medley in ccs_medlies:\n",
    "        grid[i].append(ccs_accs[to_str(medley)][val_dataset])\n",
    "ax.imshow(grid, vmin=0, vmax=1)\n",
    "for i in range(len(grid)):\n",
    "    for j in range(len(grid[0])):\n",
    "        ax.text(j, i, f'{round(grid[i][j] * 100):2d}', ha='center', va='center')\n",
    "\n",
    "ax.set_xticks(range(len(ccs_medlies)))\n",
    "ax.set_xticklabels([to_str(medley) for medley in ccs_medlies], rotation=90)\n",
    "ax.set_yticks([])\n",
    "\n",
    "\n",
    "ax = axes[3]\n",
    "ax.set_title(\"Baselines\")\n",
    "grid = [ [\n",
    "    lr_mm_accs[str(LRProbe)]['likely'][val_dataset],\n",
    "    lr_mm_accs[str(MMProbe)]['likely'][val_dataset],\n",
    "    few_shot_accs[i],\n",
    "    oracle_accs[str(LRProbe)][i]\n",
    "    ] for i, val_dataset in enumerate(val_datasets) \n",
    "]\n",
    "\n",
    "ax.imshow(grid, vmin=0, vmax=1)\n",
    "for i in range(len(grid)):\n",
    "    for j in range(len(grid[0])):\n",
    "        ax.text(j, i, f'{round(grid[i][j] * 100):2d}', ha='center', va='center')\n",
    "\n",
    "ax.set_xticks(range(4))\n",
    "ax.set_xticklabels(['LR on likely', 'MM on likely', 'calibrated 5-shot', 'LR on val set'], rotation=90)\n",
    "ax.set_yticks([])\n",
    "\n",
    "for ax in axes:\n",
    "    ax.hlines([1.5, 3.5, 5.5, 7.5], *ax.get_xlim(), linewidth=3, color='black')\n",
    "    ax.vlines([1.5], *ax.get_ylim(), linewidth=3, color='black')\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_yticks(range(len(val_datasets)))\n",
    "ax.set_yticklabels(val_datasets)\n",
    "\n",
    "\n",
    "plt.colorbar(axes[0].images[0], ax=axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
